{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50.007 Machine Learning Project\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(path, labeled=True):\n",
    "    with open(path, encoding=\"utf8\") as fp:\n",
    "        data_read = []\n",
    "        sentence = []\n",
    "        for line in fp:\n",
    "            if line == \"\\n\":\n",
    "                data_read.append(sentence)\n",
    "                sentence = []\n",
    "            else:\n",
    "                if labeled:\n",
    "                    tokens = line.strip().split()\n",
    "                    sentence.append((' '.join(tokens[:-1]), tokens[-1]))\n",
    "                else:\n",
    "                    sentence.append((line.strip()))\n",
    "    return data_read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Calculating MLE\n",
    "#### **Estimating Emission Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emissions(data):\n",
    "    label_counts = defaultdict(int)\n",
    "    emission_counts = defaultdict(lambda:defaultdict(int))\n",
    "    for sentence in data:\n",
    "        for word, label in sentence:\n",
    "            label_counts[label] += 1\n",
    "            emission_counts[label][word] += 1\n",
    "    emissions = defaultdict(float)\n",
    "    for sentence in data:\n",
    "        for word,label in sentence:\n",
    "            emission_prob = emission_counts[label][word]/label_counts[label]\n",
    "            emissions[(word, label)] = emission_prob\n",
    "    return emissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Calculating MLE\n",
    "#### **Estimating Emission Parameters accounting for unknown words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_e(data, k=1):\n",
    "    words_list = [x for sentence in data for(x,y) in sentence] \n",
    "    #all unique words in the training set\n",
    "    words = set(words_list)\n",
    "    label_counts = defaultdict(int)\n",
    "    emission_counts = defaultdict(lambda:defaultdict(int))\n",
    "    for sentence in data:\n",
    "        for word, label in sentence:\n",
    "            label_counts[label] += 1\n",
    "            emission_counts[label][word] += 1\n",
    "    emissions = defaultdict(float)\n",
    "    for sentence in data:\n",
    "        for word,label in sentence:\n",
    "            emission_prob = emission_counts[label][word]/label_counts[label]\n",
    "            emissions[(word, label)] = emission_prob\n",
    "    #this function is designed for testing phase so that we can safely handle new words that do not exist in the training set\n",
    "    def e(x,y): #x:word, y:label\n",
    "        if x not in words: \n",
    "            return k/(label_counts[y] + k)\n",
    "        else:\n",
    "            return emissions[(x,y)]\n",
    "    return e, emissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Simple Sentiment Analysis\n",
    "#### **Implementing a simple sentiment analysis system**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(data, path):\n",
    "    with open(path, \"w\", encoding='utf8') as fp:\n",
    "        for sentence in data:\n",
    "            for row in sentence:\n",
    "                fp.write(\" \".join(row)+'\\n') # rebuild the rows\n",
    "            fp.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkforUNK(dev,words):\n",
    "    dev_2 = copy.deepcopy(dev)\n",
    "    for sentence in dev_2:\n",
    "            for i, word in enumerate(sentence):\n",
    "                if word not in words:\n",
    "                    sentence[i] = \"#UNK#\"\n",
    "    return dev_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_sentiment(dataset, trainset, output_path):\n",
    "    labels_list = [y for sentence in trainset for (x,y) in sentence] \n",
    "    #all unique words in the training set\n",
    "    labels = set(labels_list)\n",
    "    words = set([x for sentence in trainset for (x,y) in sentence])\n",
    "    dataset = checkforUNK(dataset, words)\n",
    "    e, emissions = get_e(trainset)\n",
    "    output=[]\n",
    "    for sentence in dataset:\n",
    "        sentence_wlabels=[]\n",
    "        for word in sentence:\n",
    "            max_p = 0\n",
    "            label = None\n",
    "            for y in labels:\n",
    "                p = e(word,y)\n",
    "                if p >max_p:\n",
    "                    max_p = p\n",
    "                    label = y\n",
    "            sentence_wlabels.append((word,label))\n",
    "        output.append(sentence_wlabels)\n",
    "    write_file(output, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES Evaluation\n",
      "\n",
      "#Entity in gold data: 229\n",
      "#Entity in prediction: 1466\n",
      "\n",
      "#Correct Entity : 178\n",
      "Entity  precision: 0.1214\n",
      "Entity  recall: 0.7773\n",
      "Entity  F: 0.2100\n",
      "\n",
      "#Correct Sentiment : 97\n",
      "Sentiment  precision: 0.0662\n",
      "Sentiment  recall: 0.4236\n",
      "Sentiment  F: 0.1145\n"
     ]
    }
   ],
   "source": [
    "ES_dev = read_dataset('./Data/ES/dev.in', labeled=False)\n",
    "ES_data = read_dataset(\"./Data/ES/train\")\n",
    "simple_sentiment(ES_dev ,ES_data, \"Data/ES/dev.p1.out\")\n",
    "print(\"ES Evaluation\")\n",
    "!python3 EvalScript/evalResult.py Data/ES/dev.out Data/ES/dev.p1.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RU Evaluation\n",
      "\n",
      "#Entity in gold data: 389\n",
      "#Entity in prediction: 1816\n",
      "\n",
      "#Correct Entity : 266\n",
      "Entity  precision: 0.1465\n",
      "Entity  recall: 0.6838\n",
      "Entity  F: 0.2413\n",
      "\n",
      "#Correct Sentiment : 129\n",
      "Sentiment  precision: 0.0710\n",
      "Sentiment  recall: 0.3316\n",
      "Sentiment  F: 0.1170\n"
     ]
    }
   ],
   "source": [
    "RU_dev = read_dataset('./Data/RU/dev.in', labeled=False)\n",
    "RU_data = read_dataset(\"./Data/RU/train\")\n",
    "simple_sentiment(RU_dev ,RU_data, \"Data/RU/dev.p1.out\")\n",
    "print(\"RU Evaluation\")\n",
    "!python3 EvalScript/evalResult.py Data/RU/dev.out Data/RU/dev.p1.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Estimating Transition Probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_transition_parameters_test(data):\n",
    "    transition_counts = defaultdict(lambda: defaultdict(int))\n",
    "    state_counts = defaultdict(int)\n",
    "    state_counts['START'] = len(data)\n",
    "    state_counts['STOP'] = len(data)\n",
    "    # # Group sentences\n",
    "    # list_of_sentences = [list(sub) for ele, sub in groupby(sentences, key=bool) if ele]\n",
    "    # Count state and transition occurrences\n",
    "    for sentence in data:\n",
    "        prev_state = \"START\"\n",
    "        n = len(sentence)\n",
    "        for i in range(n):\n",
    "            state = sentence[i][1]\n",
    "            state_counts[state] += 1\n",
    "            if i == n-1:\n",
    "                transition_counts[prev_state][state] +=1\n",
    "                transition_counts[state][\"STOP\"] += 1\n",
    "            else:\n",
    "                transition_counts[prev_state][state] += 1 \n",
    "                prev_state = state\n",
    "    transitions = defaultdict(lambda: defaultdict(float))\n",
    "    for prev_state in transition_counts:\n",
    "        for curr_state in transition_counts[prev_state]:\n",
    "            transition_prob = transition_counts[prev_state][curr_state]/(state_counts[prev_state])\n",
    "            transitions[prev_state][curr_state] = transition_prob \n",
    "    def q(x,y):\n",
    "        return transitions[x][y]\n",
    "    return q, transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_algorithm(sentence, transition_params,e,states):\n",
    "    n = len(sentence)\n",
    "    num_states = len(states)\n",
    "    pi = [{} for i in range(n)]\n",
    "    backpointers = [{} for i in range(n)]\n",
    "\n",
    "\n",
    "    for state in states:\n",
    "        emission_prob = max(e(sentence[0],state),1e-10)\n",
    "        pi[0][state] = math.log(transition_params['START'].get(state, 1e-10)) + math.log(emission_prob)\n",
    "        backpointers[0][state] = 'START'\n",
    "\n",
    "    # Forward pass\n",
    "    for t in range(1, n):\n",
    "        for state in states:\n",
    "            max_prob = float('-inf')\n",
    "            prev_state = None\n",
    "            for prev_state in states:\n",
    "                transition_prob = transition_params[prev_state].get(state, 1e-10)\n",
    "                emission_prob = max(e(sentence[t],state),1e-10)\n",
    "                prob = pi[t - 1].get(prev_state, 1e-10) + math.log(transition_prob) + math.log(emission_prob)\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    backpointers[t][state] = prev_state\n",
    "            pi[t][state] = max_prob\n",
    "\n",
    "    # Termination step\n",
    "    max_prob = float('-inf')\n",
    "    final_state = None\n",
    "    for state in states:\n",
    "        transition_prob = transition_params[state].get('STOP', 1e-10)\n",
    "        prob = pi[n - 1][state] + math.log(transition_prob)\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            final_state = state\n",
    "\n",
    "    # Backtracking step\n",
    "    best_path = [final_state]\n",
    "    for t in range(n - 1, 0, -1):\n",
    "        best_path.insert(0, backpointers[t][best_path[0]])\n",
    "\n",
    "    return best_path\n",
    "\n",
    "# Run Viterbi algorithm on the development set using viterbi_algorithm_2\n",
    "def viterbi(dev, transition_params, e,states,words):\n",
    "    output = []\n",
    "    dev = checkforUNK(dev,words)\n",
    "    for sentence in dev:\n",
    "        best_path = viterbi_algorithm(sentence, transition_params, e, states)\n",
    "        sentence = list(zip(sentence, best_path)) # new sentence\n",
    "        output.append(sentence)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES Viterbi Evaluation\n",
      "\n",
      "#Entity in gold data: 229\n",
      "#Entity in prediction: 542\n",
      "\n",
      "#Correct Entity : 134\n",
      "Entity  precision: 0.2472\n",
      "Entity  recall: 0.5852\n",
      "Entity  F: 0.3476\n",
      "\n",
      "#Correct Sentiment : 97\n",
      "Sentiment  precision: 0.1790\n",
      "Sentiment  recall: 0.4236\n",
      "Sentiment  F: 0.2516\n"
     ]
    }
   ],
   "source": [
    "ES_dev = read_dataset('./Data/ES/dev.in', labeled=False)\n",
    "ES_data = read_dataset(\"./Data/ES/train\")\n",
    "q, transition_params = estimate_transition_parameters_test(ES_data)\n",
    "e,emissions = get_e(ES_data, k=1)\n",
    "states = set([y for sentence in ES_data for (x,y) in sentence ])\n",
    "words = set([x for sentence in ES_data for (x,y) in sentence ])\n",
    "\n",
    "output = viterbi(ES_dev, transition_params, e,states,words)\n",
    "write_file(output, './Data/ES/dev.p2.out')\n",
    "print(\"ES Viterbi Evaluation\")\n",
    "!python3 EvalScript/evalResult.py Data/ES/dev.out Data/ES/dev.p2.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RU Viterbi Evaluation\n",
      "\n",
      "#Entity in gold data: 389\n",
      "#Entity in prediction: 489\n",
      "\n",
      "#Correct Entity : 189\n",
      "Entity  precision: 0.3865\n",
      "Entity  recall: 0.4859\n",
      "Entity  F: 0.4305\n",
      "\n",
      "#Correct Sentiment : 129\n",
      "Sentiment  precision: 0.2638\n",
      "Sentiment  recall: 0.3316\n",
      "Sentiment  F: 0.2938\n"
     ]
    }
   ],
   "source": [
    "RU_dev = read_dataset('./Data/RU/dev.in', labeled=False)\n",
    "RU_data = read_dataset(\"./Data/RU/train\")\n",
    "q, transition_params = estimate_transition_parameters_test(RU_data)\n",
    "e,emissions = get_e(RU_data, k=1)\n",
    "states = set([y for sentence in RU_data for (x,y) in sentence ])\n",
    "words = set([x for sentence in RU_data for (x,y) in sentence ])\n",
    "\n",
    "output = viterbi(RU_dev, transition_params, e,states,words)\n",
    "write_file(output, './Data/RU/dev.p2.out')\n",
    "print(\"RU Viterbi Evaluation\")\n",
    "!python3 EvalScript/evalResult.py Data/RU/dev.out Data/RU/dev.p2.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Viterbi Algorithm with kth-best sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_algorithm(sentence, transition_params,e,states, k):\n",
    "    n = len(sentence)\n",
    "    num_states = len(states)\n",
    "    pi = [{} for i in range(n)]\n",
    "    backpointers = [{} for i in range(n)]\n",
    "\n",
    "\n",
    "    for state in states:\n",
    "        emission_prob = max(e(sentence[0],state),1e-10)\n",
    "        pi[0][state] = math.log(transition_params['START'].get(state, 1e-10)) + math.log(emission_prob)\n",
    "        backpointers[0][state] = 'START'\n",
    "\n",
    "    # Forward pass\n",
    "    for t in range(1, n):\n",
    "        for state in states:\n",
    "            max_prob = float('-inf')\n",
    "            prev_state = None\n",
    "            prob_list=[]\n",
    "            for prev_state in states:\n",
    "                transition_prob = transition_params[prev_state].get(state, 1e-10)\n",
    "                emission_prob = max(e(sentence[t],state),1e-10)\n",
    "                prob = pi[t - 1].get(prev_state, 1e-10) + math.log(transition_prob) + math.log(emission_prob)\n",
    "                prob_list.append((prev_state,prob))\n",
    "    \n",
    "            sorted_prob_list = sorted(prob_list, key=lambda x: x[1], reverse=True)\n",
    "            sorted_states = sorted_prob_list[:k]\n",
    "            max_prob = sorted_states[-1][1]\n",
    "            backpointers[t][state] = sorted_states[-1][0]\n",
    "            # print(backpointers[t][state])\n",
    "            pi[t][state] = max_prob\n",
    "\n",
    "    # Termination step\n",
    "    max_prob = float('-inf')\n",
    "    final_state = None\n",
    "    for state in states:\n",
    "        transition_prob = transition_params[state].get('STOP', 1e-10)\n",
    "        prob = pi[n - 1][state] + math.log(transition_prob)\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            final_state = state\n",
    "\n",
    "    # Backtracking step\n",
    "    best_path = [final_state]\n",
    "    for t in range(n - 1, 0, -1):\n",
    "        best_path.insert(0, backpointers[t][best_path[0]])\n",
    "    # print(best_path)\n",
    "    return best_path\n",
    "\n",
    "# Run Viterbi algorithm on the development set using viterbi_algorithm_2\n",
    "def modified_viterbi(dev, transition_params, e,states,words,k):\n",
    "    output = []\n",
    "    dev = checkforUNK(dev,words)\n",
    "    for sentence in dev:\n",
    "        best_path = viterbi_algorithm(sentence, transition_params, e, states,k)\n",
    "        sentence = list(zip(sentence, best_path)) # new sentence\n",
    "        output.append(sentence)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_dev = read_dataset('./Data/ES/dev.in', labeled=False)\n",
    "ES_data = read_dataset(\"./Data/ES/train\")\n",
    "q, transition_params = estimate_transition_parameters_test(ES_data)\n",
    "e,emissions = get_e(ES_data, k=1)\n",
    "states = set([y for sentence in ES_data for (x,y) in sentence ])\n",
    "words = set([x for sentence in ES_data for (x,y) in sentence ])\n",
    "\n",
    "best_output = modified_viterbi(ES_dev, transition_params, e,states,words,1)\n",
    "second_best_output = modified_viterbi(ES_dev, transition_params, e,states,words,2)\n",
    "eight_best_ouput = modified_viterbi(ES_dev, transition_params, e,states,words,8)\n",
    "\n",
    "# write_file(best_output, './Data/ES/dev.p3.1st.out')\n",
    "write_file(second_best_output, './Data/ES/dev.p3.2nd.out')\n",
    "write_file(eight_best_ouput, './Data/ES/dev.p3.8th.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Evaluation for 2nd best sequence\n",
      "\n",
      "#Entity in gold data: 229\n",
      "#Entity in prediction: 2252\n",
      "\n",
      "#Correct Entity : 123\n",
      "Entity  precision: 0.0546\n",
      "Entity  recall: 0.5371\n",
      "Entity  F: 0.0992\n",
      "\n",
      "#Correct Sentiment : 77\n",
      "Sentiment  precision: 0.0342\n",
      "Sentiment  recall: 0.3362\n",
      "Sentiment  F: 0.0621\n",
      "\n",
      "\n",
      "Evaluation for 8th best sequence\n",
      "\n",
      "#Entity in gold data: 229\n",
      "#Entity in prediction: 4046\n",
      "\n",
      "#Correct Entity : 191\n",
      "Entity  precision: 0.0472\n",
      "Entity  recall: 0.8341\n",
      "Entity  F: 0.0894\n",
      "\n",
      "#Correct Sentiment : 98\n",
      "Sentiment  precision: 0.0242\n",
      "Sentiment  recall: 0.4279\n",
      "Sentiment  F: 0.0458\n"
     ]
    }
   ],
   "source": [
    "# print(\"Evaluation for 1st best sequence\")\n",
    "# !python3 EvalScript/evalResult.py Data/ES/dev.out Data/ES/dev.p3.1st.out\n",
    "print(\"\\n\")\n",
    "print(\"Evaluation for 2nd best sequence\")\n",
    "!python3 EvalScript/evalResult.py Data/ES/dev.out Data/ES/dev.p3.2nd.out\n",
    "print(\"\\n\")\n",
    "print(\"Evaluation for 8th best sequence\")\n",
    "!python3 EvalScript/evalResult.py Data/ES/dev.out Data/ES/dev.p3.8th.out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "RU_dev = read_dataset('./Data/RU/dev.in', labeled=False)\n",
    "RU_data = read_dataset(\"./Data/RU/train\")\n",
    "q, transition_params = estimate_transition_parameters_test(RU_data)\n",
    "e,emissions = get_e(RU_data, k=1)\n",
    "states = set([y for sentence in RU_data for (x,y) in sentence ])\n",
    "words = set([x for sentence in RU_data for (x,y) in sentence ])\n",
    "\n",
    "\n",
    "best_output = modified_viterbi(RU_dev, transition_params, e,states,words,1)\n",
    "second_best_output = modified_viterbi(RU_dev, transition_params, e,states,words,2)\n",
    "eight_best_ouput = modified_viterbi(RU_dev, transition_params, e,states,words,8)\n",
    "\n",
    "write_file(second_best_output, './Data/RU/dev.p3.2nd.out')\n",
    "write_file(eight_best_ouput, './Data/RU/dev.p3.8th.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for 2nd best sequence\n",
      "\n",
      "#Entity in gold data: 389\n",
      "#Entity in prediction: 3441\n",
      "\n",
      "#Correct Entity : 163\n",
      "Entity  precision: 0.0474\n",
      "Entity  recall: 0.4190\n",
      "Entity  F: 0.0851\n",
      "\n",
      "#Correct Sentiment : 108\n",
      "Sentiment  precision: 0.0314\n",
      "Sentiment  recall: 0.2776\n",
      "Sentiment  F: 0.0564\n",
      "\n",
      "\n",
      "Evaluation for 8th best sequence\n",
      "\n",
      "#Entity in gold data: 389\n",
      "#Entity in prediction: 6152\n",
      "\n",
      "#Correct Entity : 300\n",
      "Entity  precision: 0.0488\n",
      "Entity  recall: 0.7712\n",
      "Entity  F: 0.0917\n",
      "\n",
      "#Correct Sentiment : 122\n",
      "Sentiment  precision: 0.0198\n",
      "Sentiment  recall: 0.3136\n",
      "Sentiment  F: 0.0373\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation for 2nd best sequence\")\n",
    "!python3 EvalScript/evalResult.py Data/RU/dev.out Data/RU/dev.p3.2nd.out\n",
    "print(\"\\n\")\n",
    "print(\"Evaluation for 8th best sequence\")\n",
    "!python3 EvalScript/evalResult.py Data/RU/dev.out Data/RU/dev.p3.8th.out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
