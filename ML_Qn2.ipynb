{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50.007 Machine Learning Project Part 2\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and File Reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import heapq\n",
    "from collections import defaultdict\n",
    "from itertools import groupby\n",
    "\n",
    "# Read data from files using UTF-8 encoding\n",
    "with open(\"Data/ES/train\", encoding=\"utf-8\") as f:\n",
    "    es = f.read().splitlines()\n",
    "with open(\"Data/RU/train\", encoding=\"utf-8\") as f:\n",
    "    ru = f.read().splitlines()\n",
    "with open(\"Data/ES/dev.in\", encoding=\"utf-8\") as f:\n",
    "    dev_in_es = f.read().splitlines()\n",
    "with open(\"Data/ES/dev.out\", encoding=\"utf-8\") as f:\n",
    "    dev_out_es = f.read().splitlines()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating Emission Parameters for Hidden Markov Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emission Parameters for ('palo', 'O'):  3.238656605240146e-05\n",
      "States: ['O', 'B-positive', 'B-negative', 'B-neutral', 'I-neutral', 'I-positive', 'I-negative']\n"
     ]
    }
   ],
   "source": [
    "def estimate_emission_params(train_data, k=1):\n",
    "    word_sentiment_counts = defaultdict(lambda: defaultdict(int))\n",
    "    sentiment_counts = defaultdict(int)\n",
    "    emission_params = {}\n",
    "\n",
    "    # Count occurrences of (label, word) pairs\n",
    "    for sentence in train_data:\n",
    "        try:\n",
    "            if sentence != \"\":\n",
    "                x, label = sentence.split(\" \")\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        sentiment_counts[label] += 1\n",
    "        word_sentiment_counts[label][x] += 1\n",
    "\n",
    "    # Calculate emission parameters\n",
    "    for key in word_sentiment_counts:\n",
    "        for word in word_sentiment_counts[key]:\n",
    "            emission_params[(word, key)] = word_sentiment_counts[key][word] / sentiment_counts[key]\n",
    "\n",
    "    return emission_params, sentiment_counts\n",
    "\n",
    "# Estimate emission parameters\n",
    "es_para, count = estimate_emission_params(es)\n",
    "print(\"Emission Parameters for ('palo', 'O'): \", es_para[(\"palo\", \"O\")])\n",
    "\n",
    "# Extract states\n",
    "states = list(count.keys())\n",
    "print(\"States:\", states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating Transition Probabilities for Hidden Markov Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition Parameters: {'START': {'O': 0.9289176090468497, 'B-positive': 0.052234787291330104, 'B-negative': 0.014001077005923533, 'B-neutral': 0.004846526655896607}, 'O': {'O': 0.9456845511712573, 'B-positive': 0.038980620012503214, 'END': 0.06773802081418012, 'B-negative': 0.013054830287206266, 'B-neutral': 0.002279998529033207}, 'B-positive': {'O': 0.8791304347826087, 'I-positive': 0.11739130434782609, 'END': 0.008695652173913044, 'B-neutral': 0.0008695652173913044, 'B-positive': 0.0026086956521739132}, 'B-negative': {'O': 0.8196286472148541, 'I-negative': 0.18037135278514588, 'END': 0.010610079575596816}, 'B-neutral': {'I-neutral': 0.20833333333333334, 'O': 0.7916666666666666}, 'I-neutral': {'I-neutral': 0.6511627906976745, 'O': 0.3488372093023256}, 'I-positive': {'I-positive': 0.5718849840255591, 'O': 0.4281150159744409, 'END': 0.003194888178913738}, 'I-negative': {'O': 0.39766081871345027, 'I-negative': 0.6023391812865497}}\n"
     ]
    }
   ],
   "source": [
    "def estimate_transition_parameters_test(sentences):\n",
    "    transition_counts = {}\n",
    "    state_counts = {}\n",
    "\n",
    "    # Group sentences\n",
    "    list_of_sentences = [list(sub) for ele, sub in groupby(sentences, key=bool) if ele]\n",
    "\n",
    "    # Count state and transition occurrences\n",
    "    for one_sentence in list_of_sentences:\n",
    "        prev_state = 'START'\n",
    "\n",
    "        for one_word in one_sentence:\n",
    "            if one_word != \"\":\n",
    "                word, state = one_word.split(\" \")\n",
    "\n",
    "                # Count state occurrences\n",
    "                if state_counts.get(prev_state):\n",
    "                    state_counts[prev_state] += 1\n",
    "                else:\n",
    "                    state_counts[prev_state] = 1\n",
    "\n",
    "                # Count transition occurrences\n",
    "                if prev_state not in transition_counts:\n",
    "                    transition_counts[prev_state] = {}\n",
    "                if state not in transition_counts[prev_state]:\n",
    "                    transition_counts[prev_state][state] = 1\n",
    "                else:\n",
    "                    transition_counts[prev_state][state] += 1\n",
    "\n",
    "                prev_state = state\n",
    "\n",
    "        # Count 'END' transitions\n",
    "        if \"END\" not in transition_counts[prev_state]:\n",
    "            transition_counts[prev_state][\"END\"] = 1\n",
    "        else:\n",
    "            transition_counts[prev_state][\"END\"] += 1\n",
    "\n",
    "    # Calculate transition probabilities\n",
    "    for from_state, to_states in transition_counts.items():\n",
    "        total_from_state_count = state_counts[from_state]\n",
    "        for to_state, count in to_states.items():\n",
    "            transition_counts[from_state][to_state] = count / total_from_state_count\n",
    "\n",
    "    return transition_counts\n",
    "\n",
    "# Estimate transition parameters\n",
    "transition_params = estimate_transition_parameters_test(es)\n",
    "print(\"Transition Parameters:\", transition_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhanced Viterbi Algorithm with Backpointers for Sequence Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertibi_algorithm_test(sentence, transition_params, emission_params, states):\n",
    "    n = len(sentence)\n",
    "    num_states = len(states)\n",
    "    viterbi = [{} for _ in range(n)]\n",
    "    backpointers = [{} for _ in range(n)]\n",
    "\n",
    "    # Initialization at time step 0\n",
    "    word = sentence[0]\n",
    "    for i in range(num_states):\n",
    "        viterbi[0][states[i]] = math.log(emission_params.get((word, states[i]), 1e-10) + transition_params[\"START\"].get(states[i], 1e-10))\n",
    "    \n",
    "    # Forward pass\n",
    "    for z in range(1, n):\n",
    "        word = sentence[z]\n",
    "        for i in range(num_states):\n",
    "            max_prob = float('-inf')\n",
    "            save_state = \"\"\n",
    "            for j in range(num_states):\n",
    "                prob = viterbi[z - 1].get(states[j], float('-inf')) + math.log(emission_params.get((word, states[i]), 1e-10) + transition_params[states[j]].get(states[i], 1e-10))\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    save_state = states[j]\n",
    "            viterbi[z][states[i]] = max_prob\n",
    "            backpointers[z][states[i]] = save_state\n",
    "\n",
    "    # Termination step\n",
    "    max_prob = float('-inf')\n",
    "    final_state = 'STOP'\n",
    "    for j in range(num_states):\n",
    "        prob = viterbi[n - 1].get(states[j], float('-inf')) + math.log(transition_params[states[j]].get(\"STOP\", 1e-10))\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            final_state = states[j]\n",
    "    \n",
    "    # Backtracking step\n",
    "    best_path = [final_state]\n",
    "    for m in range(n - 1, 0, -1):\n",
    "        best_path.insert(0, backpointers[m][best_path[0]])\n",
    "    \n",
    "    return best_path\n",
    "\n",
    "def run_viterbi_on_dev_set(dev_set, transition_params, emission_params, states):\n",
    "    output = []\n",
    "    list_of_sentences = [list(sub) for ele, sub in groupby(dev_set, key=bool) if ele]\n",
    "    for sentence in list_of_sentences:\n",
    "        best_path = vertibi_algorithm_test(sentence, transition_params, emission_params, states)\n",
    "        output.append(best_path)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(true_tags, predicted_tags):\n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    \n",
    "    for true_seq, pred_seq in zip(true_tags, predicted_tags):\n",
    "        for true, pred in zip(true_seq, pred_seq):\n",
    "            # Print for debugging purposes\n",
    "            print(true, pred, true == pred)\n",
    "            \n",
    "            if true == pred and true != 'O':\n",
    "                true_positive += 1\n",
    "                print(\"True Positive Count:\", true_positive)\n",
    "            elif true != pred and true != 'O' and pred != 'O':\n",
    "                false_positive += 1\n",
    "                false_negative += 1  # Counting false negatives\n",
    "    \n",
    "    print(\"True Positive:\", true_positive)\n",
    "    print(\"False Positive:\", false_positive)\n",
    "    print(\"False Negative:\", false_negative)\n",
    "    \n",
    "    # Calculate precision, recall, and F-score\n",
    "    precision = true_positive / (true_positive + false_positive) if true_positive + false_positive > 0 else 0\n",
    "    recall = true_positive / (true_positive + false_negative) if true_positive + false_negative > 0 else 0\n",
    "    f_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    \n",
    "    return precision, recall, f_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and F-score Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision from true and predicted labels\n",
    "def calculate_precision(true_labels, predicted_labels):\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    for true_seq, pred_seq in zip(true_labels, predicted_labels):\n",
    "        for true, pred in zip(true_seq, pred_seq):\n",
    "            if true == pred:\n",
    "                correct_count += 1\n",
    "            total_count += 1\n",
    "    \n",
    "    precision = correct_count / total_count if total_count > 0 else 0\n",
    "    return precision\n",
    "\n",
    "# Calculate F-score using recall and precision\n",
    "def calculate_f_score(recall, precision):\n",
    "    v1 = 1 / precision if precision > 0 else 1\n",
    "    v2 = 1 / recall if recall > 0 else 1\n",
    "    f_score = 2 / (v1 + v2) if (v1 + v2) > 0 else 0\n",
    "    return f_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhanced Viterbi Algorithm with Backpointers for Analysis Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi Algorithm Implementation with Backpointers\n",
    "def viterbi_algorithm_2(sentence, transition_params, emission_params, states):\n",
    "    n = len(sentence)\n",
    "    num_states = len(states)\n",
    "    viterbi = [{} for _ in range(n)]\n",
    "    backpointers = [{} for _ in range(n)]\n",
    "\n",
    "    # Initialization at time step 0\n",
    "    for state in states:\n",
    "        emission_prob = emission_params.get((sentence[0], state), 1e-10)\n",
    "        viterbi[0][state] = math.log(transition_params['START'].get(state, 1e-10)) + math.log(emission_prob)\n",
    "        backpointers[0][state] = 'START'\n",
    "\n",
    "    # Forward pass\n",
    "    for t in range(1, n):\n",
    "        for state in states:\n",
    "            max_prob = float('-inf')\n",
    "            prev_state = None\n",
    "            for prev_state in states:\n",
    "                transition_prob = transition_params[prev_state].get(state, 1e-10)\n",
    "                emission_prob = emission_params.get((sentence[t], state), 1e-10)\n",
    "                prob = viterbi[t - 1].get(prev_state, 1e-10) + math.log(transition_prob) + math.log(emission_prob)\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    backpointers[t][state] = prev_state\n",
    "            viterbi[t][state] = max_prob\n",
    "\n",
    "    # Termination step\n",
    "    max_prob = float('-inf')\n",
    "    final_state = None\n",
    "    for state in states:\n",
    "        transition_prob = transition_params[state].get('STOP', 1e-10)\n",
    "        prob = viterbi[n - 1][state] + math.log(transition_prob)\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            final_state = state\n",
    "\n",
    "    # Backtracking step\n",
    "    best_path = [final_state]\n",
    "    for t in range(n - 1, 0, -1):\n",
    "        best_path.insert(0, backpointers[t][best_path[0]])\n",
    "\n",
    "    return best_path\n",
    "\n",
    "# Run Viterbi algorithm on the development set using viterbi_algorithm_2\n",
    "def run_viterbi_on_dev_set_2(dev_set, transition_params, emission_params, states):\n",
    "    output = []\n",
    "    list_of_sentences = [list(sub) for ele, sub in groupby(dev_set, key=bool) if ele]\n",
    "    for sentence in list_of_sentences:\n",
    "        best_path = viterbi_algorithm_2(sentence, transition_params, emission_params, states)\n",
    "        output.append(best_path)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision using implementation 1: 0.9271799628942486\n",
      "Precision using implementation 2: 0.935064935064935\n"
     ]
    }
   ],
   "source": [
    "# Train the model on the training set\n",
    "transition_params = estimate_transition_parameters_test(es)\n",
    "emission_params, count = estimate_emission_params(es)\n",
    "\n",
    "# Extract the list of states from the count dictionary\n",
    "states = list(count.keys())\n",
    "\n",
    "# Run Viterbi algorithm on the development set using both implementations\n",
    "predicted_tags = run_viterbi_on_dev_set(dev_in_es, transition_params, emission_params, states)\n",
    "predicted_tags_2 = run_viterbi_on_dev_set_2(dev_in_es, transition_params, emission_params, states)\n",
    "\n",
    "# Function to extract actual tags from a test set\n",
    "def actual_tags(test_set):\n",
    "    tags = []\n",
    "    list_of_sentences = [list(sub) for ele, sub in groupby(test_set, key=bool) if ele]\n",
    "    for sentence in list_of_sentences:\n",
    "        innerlist = []\n",
    "        for word in sentence:\n",
    "            w, state = word.split()\n",
    "            innerlist.append(state)\n",
    "        tags.append(innerlist)\n",
    "    return tags\n",
    "\n",
    "# Calculate precision using the scores function\n",
    "precision = calculate_precision(actual_tags(dev_out_es), predicted_tags)\n",
    "print(\"Precision using implementation 1:\", precision)\n",
    "\n",
    "precision_2 = calculate_precision(actual_tags(dev_out_es), predicted_tags_2)\n",
    "print(\"Precision using implementation 2:\", precision_2)\n",
    "\n",
    "# The following lines are commented out as they depend on the compute_metrics function:\n",
    "# precision, recall, f_score = compute_metrics(actual_tags(dev_out_es), predicted_tags)\n",
    "# print(\"Precision:\", precision)\n",
    "# print(\"Recall:\", recall)\n",
    "# print(\"F-score:\", f_score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
